{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.functional import softmax\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\nfrom tqdm import tqdm\nimport ast\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import TensorDataset\n\nimport os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\ndf1 = pd.read_csv(\"/kaggle/input/tweet-analysis/tweet_sentiment.csv\") \ndf2 = pd.read_csv(\"/kaggle/input/corona-data/Corona.csv\")                 \ndf3 = pd.read_csv(\"/kaggle/input/dell-tweets/Dell_tweets.csv\")        \ndf4 = pd.read_csv(\"/kaggle/input/some-sentiment/twitter1.csv\")       \ndf5 = pd.read_csv(\"/kaggle/input/tweets-cleaned/Tweets_cleaned.csv\") \n\ndf_master = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\ndf_master.dropna(subset=['tweet', 'sentiment'], inplace=True)\n\nprint(df_master.columns)\n\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\ndevice = torch.device(\"cpu\")\nmodel.to(device)\nmodel.eval()\n\n# Labels\ngoemotion_labels = [\n    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n    'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n    'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',\n    'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n]\n\ngoemotion_to_sentiment = {\n    'admiration': 'positive', 'amusement': 'positive', 'approval': 'positive',\n    'caring': 'positive', 'curiosity': 'positive', 'desire': 'positive', 'excitement': 'positive',\n    'gratitude': 'positive', 'joy': 'positive', 'love': 'positive', 'optimism': 'positive',\n    'pride': 'positive', 'relief': 'positive', 'realization': 'positive',\n\n    'anger': 'negative', 'annoyance': 'negative', 'disapproval': 'negative',\n    'disappointment': 'negative', 'disgust': 'negative', 'embarrassment': 'negative',\n    'fear': 'negative', 'grief': 'negative', 'nervousness': 'negative', 'remorse': 'negative',\n    'sadness': 'negative', 'confusion': 'negative',\n\n    'surprise': 'neutral', 'neutral': 'neutral'\n}\n\nplutchik_map = {\n    'joy': 'joy', 'amusement': 'joy', 'excitement': 'joy', 'pride': 'joy', 'gratitude': 'joy',\n    'love': 'trust', 'admiration': 'trust', 'caring': 'trust', 'approval': 'trust',\n    'optimism': 'anticipation', 'realization': 'anticipation', 'curiosity': 'anticipation', 'desire': 'anticipation',\n    'fear': 'fear', 'nervousness': 'fear',\n    'remorse': 'sadness', 'grief': 'sadness', 'sadness': 'sadness', 'disappointment': 'sadness',\n    'anger': 'anger', 'annoyance': 'anger', 'disapproval': 'anger',\n    'confusion': 'surprise', 'surprise': 'surprise', 'embarrassment': 'surprise',\n    'disgust': 'disgust', 'relief': 'trust', 'neutral': 'neutral'\n}\n\nekman_map = {\n    'anger': 'anger', 'annoyance': 'anger', 'disapproval': 'anger', 'remorse': 'anger',\n    'fear': 'fear', 'nervousness': 'fear',\n    'disgust': 'disgust',\n    'joy': 'happiness', 'amusement': 'happiness', 'excitement': 'happiness', 'gratitude': 'happiness', 'pride': 'happiness',\n    'love': 'happiness', 'relief': 'happiness', 'admiration': 'happiness', 'caring': 'happiness', 'approval': 'happiness',\n    'sadness': 'sadness', 'grief': 'sadness', 'disappointment': 'sadness',\n    'surprise': 'surprise', 'confusion': 'surprise', 'embarrassment': 'surprise',\n    'optimism': 'neutral', 'realization': 'neutral', 'curiosity': 'neutral', 'desire': 'neutral', 'neutral': 'neutral'\n}\n\ndef predict_goemotions(text):\n    if not isinstance(text, str):\n        text = str(text)  # convert to string if not already\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    probs = torch.sigmoid(logits)[0]\n    threshold = 0.3  # adjust threshold as needed\n    id2label = model.config.id2label\n    predicted_labels = [id2label[i] for i, p in enumerate(probs) if p > threshold]\n    return predicted_labels\n\n# Dataset class for training and validation\nclass TweetDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.encodings = tokenizer(texts.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\n\ntexts = df_master['tweet']  # Assuming df_master is your DataFrame\nlabels = df_master['sentiment']\n\n# Split data into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\ntrain_texts_clean = train_texts.str.lower()  # Apply your custom cleaning\nval_texts_clean = val_texts.str.lower()  # Apply your custom cleaning\n\n# Tokenize the texts using BERT's tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntrain_encodings = tokenizer(list(train_texts_clean), truncation=True, padding=True, return_tensors=\"pt\")\nval_encodings = tokenizer(list(val_texts_clean), truncation=True, padding=True, return_tensors=\"pt\")\n\n# Encode the labels\nlabel_encoder = LabelEncoder()\ndf_master['sentiment_encoded'] = label_encoder.fit_transform(df_master['sentiment'])\nlookup = dict(zip(df_master['tweet'], df_master['sentiment_encoded']))\ntrain_labels_encoded = label_encoder.fit_transform(train_labels)\nval_labels_encoded = label_encoder.transform(val_labels)\n\n# Extract input IDs and attention masks from the tokenized data\ntrain_input_ids = train_encodings['input_ids']\ntrain_attention_mask = train_encodings['attention_mask']\nval_input_ids = val_encodings['input_ids']\nval_attention_mask = val_encodings['attention_mask']\n\n# Convert labels to torch tensors\ntrain_labels_tensor = torch.tensor(train_labels_encoded.tolist(), dtype=torch.long)\nval_labels_tensor = torch.tensor(val_labels_encoded.tolist(), dtype=torch.long)\n\n# Create TensorDataset for training and validation\ntrain_dataset = TensorDataset(\n    train_input_ids,\n    train_attention_mask,\n    train_labels_tensor\n)\n\nval_dataset = TensorDataset(\n    val_input_ids,\n    val_attention_mask,\n    val_labels_tensor\n)\n\n# Optionally, create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\nprint(set(train_labels_tensor.tolist()))\nprint(set(val_labels_tensor.tolist()))\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n    for batch in loop:\n        for batch in loop:\n            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n            token_type_ids = torch.zeros_like(input_ids)\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,  # Safe\n                labels=labels\n            )\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            loop.set_postfix(loss=loss.item())\n\n    # Validation phase\n    model.eval()\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            for batch in loop:\n                input_ids, attention_mask, labels = [x.to(device) for x in batch]\n                outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,  # Safe\n                labels=labels\n                )\n                logits = outputs.logits\n                preds = torch.argmax(logits, dim=-1)\n                val_preds.extend(preds.cpu().numpy())\n                val_labels.extend(batch['labels'].cpu().numpy())\n\n    # Evaluation metrics\n    print(f\"Epoch {epoch + 1} - Validation Accuracy:\", accuracy_score(val_labels, val_preds))\n    print(f\"Epoch {epoch + 1} - Validation F1 Score (macro):\", f1_score(val_labels, val_preds, average='macro'))\n    print(f\"Epoch {epoch + 1} - Validation Precision (macro):\", precision_score(val_labels, val_preds, average='macro'))\n    print(f\"Epoch {epoch + 1} - Validation Recall (macro):\", recall_score(val_labels, val_preds, average='macro'))\n\ndef plot_distribution(emotion_lists, title, color):\n    flat = [e for sublist in emotion_lists for e in sublist]\n    counts = dict(Counter(flat))\n    sorted_items = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True))\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=list(sorted_items.keys()), y=list(sorted_items.values()), color=color)\n    plt.title(title)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\nplot_distribution(df_master[\"predicted_goemotions\"], \"GoEmotions Distribution\", \"skyblue\")\nplot_distribution(df_master[\"plutchik_map\"], \"Plutchik Emotions Distribution\", \"salmon\")\nplot_distribution(df_master[\"ekman_map\"], \"Ekman Emotions Distribution\", \"limegreen\")\n\n\ndf_master[\"predicted_goemotions\"] = df_master[\"tweet\"].apply(predict_goemotions)\ndf_master[\"predicted_goemotions\"] = df_master[\"predicted_goemotions\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\ndf_master[\"pred_top1\"] = df_master[\"predicted_goemotions\"].apply(\n    lambda x: x[0]['label'] if isinstance(x, list) and len(x) > 0 and isinstance(x[0], dict) and 'label' in x[0] else \"neutral\"\n)\n\ny_true = []\ny_pred = []\n\ndef predict_emotion(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n    with torch.no_grad():\n        logits = model(**inputs).logits\n        probs = softmax(logits, dim=1)[0]\n\n    top5_probs, top5_indices = torch.topk(probs, 5)\n    top5_emotions = [(goemotion_labels[idx], prob.item()) for idx, prob in zip(top5_indices, top5_probs)]\n\n    # Dominant emotion\n    top_emotion = top5_emotions[0][0]\n    ekman_emotion = ekman_map.get(top_emotion, 'other')\n    plutchik_emotion = plutchik_map.get(top_emotion, 'other')\n\n    return top_emotion, ekman_emotion, plutchik_emotion, top5_emotions\n\n# --- Test with a Custom Sentence ---\ntext = input(\"📝 Enter a sentence: \")\ntop_goemotion, ekman, plutchik, top5 = predict_emotion(text)\n\n# --- Display Results ---\nprint(\"\\n📢 Predicted Emotions:\")\nprint(f\"GoEmotion: {top_goemotion}\")\nprint(f\"Mapped Ekman Emotion: {ekman}\")\nprint(f\"Mapped Plutchik Emotion: {plutchik}\\n\")\n\nprint(\"🎯 Top 5 Predicted GoEmotions:\")\nfor emo, prob in top5:\n    print(f\" - {emo}: {prob:.4f}\")\n\nprint(\"🔍 Running predictions...\")\nfor _, row in tqdm(df_master.iterrows(), total=len(df_master)):\n    text = row['tweet']\n    true_label = row['sentiment']\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n    \n    with torch.no_grad():\n        logits = model(**inputs).logits\n        probs = softmax(logits, dim=1)[0]\n        pred_idx = torch.argmax(probs).item()\n        goem_label = goemotion_labels[pred_idx]\n        pred_label = goemotion_to_sentiment.get(goem_label, 'neutral')\n    \n    y_true.append(true_label)\n    y_pred.append(pred_label)\n\n# 🔢 Evaluation Metrics\nprint(\"Accuracy:\", accuracy_score(y_true, y_pred))\nprint(\"F1 Score (macro):\", f1_score(y_true, y_pred, average='macro'))\nprint(\"Precision (macro):\", precision_score(y_true, y_pred, average='macro'))\nprint(\"Recall (macro):\", recall_score(y_true, y_pred, average='macro'))\n\n# Detailed classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T23:33:04.187808Z","iopub.execute_input":"2025-04-18T23:33:04.188489Z"}},"outputs":[{"name":"stderr","text":"2025-04-18 23:33:17.739407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745019197.922729      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745019197.982279      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Index(['tweet', 'sentiment'], dtype='object')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfe4f3bcef88463e96eca54ad8b5a910"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b96518ce019e497da472f31681ccff46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6ea11693ab44886b36dd13fbfebea8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"772ae397138048f88a211c643f43a422"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"495e8b84b2744e82b9c74f8485e96953"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{0, 1, 2, 3}\n{0, 1, 2, 3}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:   5%|▌         | 316/6204 [42:40<13:46:38,  8.42s/it, loss=0.889]","output_type":"stream"}],"execution_count":null}]}